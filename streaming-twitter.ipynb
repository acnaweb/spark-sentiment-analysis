{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streming Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pacotes de autenticacao\n",
    "#!pip install requests_oauthlib\n",
    "#!pip install twython\n",
    "#!pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando modulos\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import oauth1_session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package requests_oauthlib:\n",
      "\n",
      "NAME\n",
      "    requests_oauthlib\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    compliance_fixes (package)\n",
      "    oauth1_auth\n",
      "    oauth1_session\n",
      "    oauth2_auth\n",
      "    oauth2_session\n",
      "\n",
      "VERSION\n",
      "    1.3.0\n",
      "\n",
      "FILE\n",
      "    /home/ac/anaconda3/lib/python3.7/site-packages/requests_oauthlib/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(\"requests_oauthlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencia de updates\n",
    "INTERVAL_BATCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o StreamingContext\n",
    "ssc = StreamingContext(sc, INTERVAL_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o classificador\n",
    "* 1 sentimento positivo\n",
    "* 0 sentimento negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/dataset_analise_sentimento.csv\"\n",
    "if os.path.isfile(filename):\n",
    "    ds = sc.textFile(filename)\n",
    "else:\n",
    "    print(\"arquivo inexistente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ItemID,Sentiment,SentimentSource,SentimentText',\n",
       " '1,0,Sentiment140,                     is so sad for my APL friend.............',\n",
       " '2,0,Sentiment140,                   I missed the New Moon trailer...',\n",
       " '3,1,Sentiment140,              omg its already 7:30 :O',\n",
       " \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ds.first()\n",
    "dataset = ds.filter(lambda line : line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
    "def get_row(line):\n",
    "    row = line.split(\",\")\n",
    "    sentimento = row[1]\n",
    "    tweet = row[3].strip()\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(\" \")\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "        tweet_lower.append(word.lower())\n",
    "    return (tweet_lower, sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino = dataset.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/ntlkdata.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = \"images/ntlkdata.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_all = []\n",
    "for word in stopwords.words(\"english\"):\n",
    "    stopwords_all.append(word)\n",
    "    stopwords_all.append(word + \"_NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino_amostra = dataset_treino.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.collections.LazyMap"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence1 = [([\"this\", \"program\", \"is\", \"bad\"], \"\")]\n",
    "test_sentence2 = [([\"tough\", \"day\", \"at\", \"work\", \"today\"], \"\")]\n",
    "test_sentence3 = [([\"good\", \"wonderful\", \"amazing\", \"awesome\"], \"\")]\n",
    "\n",
    "test_set1 = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"J1sHChMSqrr5as1jtmP9Duo2bV\"\n",
    "consumer_secret = \"cg25DJ7thm1pgnWW1VrBVRmDQtmwefNeQOJxxf1Z0GSgg4tiPPK\"\n",
    "access_key = \"25630007515-sIbKfRbuAbIftp71x0pc0cXgt5uUWZgPTaqvswu\"\n",
    "acesss_token_secret = \"VIpm4tHmO9zfUGFDaK6Oz19Kjeqw9UyAmdgkEd6umxETFN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Lamar\"\n",
    "sample_url = \"https://stream.twitter.com/1.1/statuses/samples.json\"\n",
    "filter_url = \"https://stream.twitter.com/1.1/statuses/filter.json?track=\"+search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_key, acesss_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = ssc.sparkContext.parallelize([0])\n",
    "stream = ssc.queueStream([], default = rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TWEETS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfunc(t, rdd):\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "    response = requests.get(filter_url, auth = auth, stream = True)\n",
    "    print(filter_url, response)\n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        try:\n",
    "            if count > NUM_TWEETS:\n",
    "                break\n",
    "            post  = json.loads(line.decode(\"utf-8\"))\n",
    "            contents = [post[\"text\"]]\n",
    "            count += 1\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            result = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(tfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifica_tweet(tweet):\n",
    "    sentece = [(tweet, \"\")]\n",
    "    test_set = sentiment_analyzer.apply_features(sentence)\n",
    "    print(tweet, classifier.classify(test_set[0][0]))\n",
    "    return (tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_text(rdd):\n",
    "    for line in rdd:\n",
    "        tweet = line.strip()\n",
    "        translator = str.maketrans({key: None for key in string.punctuation})\n",
    "        tweet = tweet.translate(translator)\n",
    "        tweet = tweet.split(\" \")\n",
    "        tweet_lower = []\n",
    "        for word in tweet:\n",
    "            tweet_lower.append(word.lower())\n",
    "        return (classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_rdd(rdd):\n",
    "    global resultados\n",
    "    pairs = rdd.map(lambda x: (get_tweet_text(x)[1], 1))\n",
    "    counts = pairs.reduceByKey(add)\n",
    "    output = []\n",
    "    for count in counts.collect():\n",
    "        output.append(count)\n",
    "    result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "    resultados.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "    if len(resultados) > 5:\n",
    "        cont = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdd_save = \"data/result\";time.strftime(\"%I%M%S\")\n",
    "resuldados_rdd = sc.parallelize(resultados)\n",
    "resuldados_rdd.saveTextFile(rdd_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resuldados_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
